# -*- coding: utf-8 -*-
"""Polyp_AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10zCHI_JFIccR1poKbbRsr7s4Sftlzw5V
"""

###Here is the code used to create Polyp AI, a CNN based model used to categorize polyps during colonoscopies. Data was downloaded from the PolypGen database
###Lines 13 - 64 and 470 - end of file contain all code used to train the model
###The commented section between 64 - 470 was my attempt at implementing an Object detection model with the use of CNNs that outputs masks.
###Use your own API token on line 40 to extract the data from synapse client

import torch
from torch import nn

#Device agnostic code
device = "cuda" if torch.cuda.is_available() else "cpu"

"""###Installing synapseclient to download PolypGen database from Synapse biomedical database

If asked to restart session, press cancel
"""

!pip install synapseclient

import synapseclient
from pathlib import Path



# Define the destination path for the downloaded file
data_path = Path("data")

import synapseclient
from synapseclient import File

# Log in to Synapse
syn = synapseclient.Synapse()
syn.login(authToken='YOUR_API_TOKEN')


# Define the Synapse ID of the file to download
synapse_id = 'syn45200214'
file_entity = syn.get(synapse_id, downloadLocation=str(data_path))
print(f"Downloaded file to {file_entity.path}")

  # Unzip the downloaded file if it's a zip file
if not data_path.is_dir():
    data_path.mkdir(parents=True, exist_ok=True)
destination_path = data_path / f"{synapse_id}.zip"

# Download the file
polypGen_dir = data_path / "PolypGen2021_MultiCenterData_v3"

if not polypGen_dir.is_dir():

  import zipfile

  try:
      with zipfile.ZipFile(file_entity.path, "r") as zip_ref:
          zip_ref.extractall(data_path)
          print("Unzipping completed!")
  except zipfile.BadZipFile:
      print(f"The downloaded file is not a valid zip file: {file_entity.path}")
else:
  print(f"{polypGen_dir} directory already exists")

# import random
# from pathlib import Path
# from sklearn.model_selection import train_test_split

# # Define image and label paths
# image_paths = [
#     Path("data/PolypGen2021_MultiCenterData_v3/data_C1/images_C1"),
#     Path("data/PolypGen2021_MultiCenterData_v3/data_C2/images_C2"),
#     Path("data/PolypGen2021_MultiCenterData_v3/data_C3/images_C3"),
#     Path("data/PolypGen2021_MultiCenterData_v3/data_C4/images_C4"),
#     Path("data/PolypGen2021_MultiCenterData_v3/data_C5/images_C5"),
#     Path("data/PolypGen2021_MultiCenterData_v3/data_C6/images_C6")
# ]

# label_paths = [
#     Path("data/PolypGen2021_MultiCenterData_v3/data_C1/masks_C1"),
#     Path("data/PolypGen2021_MultiCenterData_v3/data_C2/masks_C2"),
#     Path("data/PolypGen2021_MultiCenterData_v3/data_C3/masks_C3"),
#     Path("data/PolypGen2021_MultiCenterData_v3/data_C4/masks_C4"),
#     Path("data/PolypGen2021_MultiCenterData_v3/data_C5/masks_C5"),
#     Path("data/PolypGen2021_MultiCenterData_v3/data_C6/masks_C6")
# ]

# # Combine all image and label files
# all_image_files = []
# all_label_files = []

# for image_path, label_path in zip(image_paths, label_paths):
#     image_files = sorted(image_path.glob("*.jpg"))
#     label_files = sorted(label_path.glob("*.jpg"))

#     assert len(image_files) == len(label_files), f"Number of images and masks do not match in {image_path} and {label_path}"

#     all_image_files.extend(image_files)
#     all_label_files.extend(label_files)

# # Split into train and test sets (80% train, 20% test)
# train_images, test_images, train_labels, test_labels = train_test_split(all_image_files, all_label_files, test_size=0.2, random_state=42)

# print(f"Training set: {len(train_images)} images")
# print(f"Test set: {len(test_images)} images")

"""Function to apply transforms for the data"""

# import torch
# from torch.utils.data import DataLoader
# from torchvision import datasets, transforms
# from PIL import Image
# import matplotlib.pyplot as plt


# transform = transforms.Compose([
#     transforms.Resize((128, 128)),  # Resize images and masks to 256x256 pixels
#     # transforms.RandomHorizontalFlip(p=0.5), #Randomly flips images and masks horizonatally
#     # transforms.RandomVerticalFlip(p=0.5), #Randomly flips images and masks vertically
#     transforms.ToTensor()           # Convert images and masks to tensors
# ])

"""##Loading and transforming the image and mask"""

# def load_and_transform_image(image_path, mask_path, transform):
#     image = Image.open(image_path).convert("RGB")
#     mask = Image.open(mask_path).convert("L")  # Convert mask to grayscale

#     image = transform(image)
#     mask = transform(mask)

#     return image, mask

# # Custom collate function

# BATCH_SIZE = 16
# def custom_collate_fn(batch):
#     images = []
#     masks = []

#     for img_path, mask_path in batch:
#         image, mask = load_and_transform_image(img_path, mask_path, transform)
#         images.append(image)
#         masks.append(mask)

#     return torch.stack(images), torch.stack(masks)

# # Combine image and label paths into tuples
# train_data = list(zip(train_images, train_labels))
# test_data = list(zip(test_images, test_labels))

# # Create DataLoaders
# train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)
# test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)

# # Function to display a random image and mask from the DataLoader
# def display_random_image_from_loader(loader):
#     # Get a random batch of images and masks
#     images, masks = next(iter(loader))

#     # Select a random image and its mask from the batch
#     idx = random.randint(0, images.size(0) - 1)
#     image = images[idx].permute(1, 2, 0).numpy()  # Convert from CxHxW to HxWxC
#     mask = masks[idx].squeeze(0).numpy()  # Remove channel dimension

#     # Display the image and mask
#     fig, ax = plt.subplots(1, 2, figsize=(12, 6))
#     ax[0].imshow(image)
#     ax[0].set_title("Image")
#     ax[0].axis('off')

#     ax[1].imshow(mask, cmap='gray')
#     ax[1].set_title("Mask")
#     ax[1].axis('off')

#     plt.show()

#     print(image.shape)
#     print(mask.shape)
#     print(mask)

# # Display a random image with its mask from the training set
# display_random_image_from_loader(train_loader)

"""##Creating a CNN"""

# import torch
# from torch import nn
# import matplotlib.pyplot as plt

# class CNN_Model_V0(nn.Module):
#     def __init__(self, input_channels: int, hidden_units: int, output_channels: int):
#         super().__init__()

#         self.conv_block_1 = nn.Sequential(
#             nn.Conv2d(in_channels=input_channels, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),
#             nn.BatchNorm2d(hidden_units),
#             nn.ReLU(),
#             nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),
#             nn.BatchNorm2d(hidden_units),
#             nn.ReLU(),
#             nn.MaxPool2d(kernel_size=2)  # Reduces spatial dimensions by a factor of 2
#         )

#         self.conv_block_2 = nn.Sequential(
#             nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units*2, kernel_size=3, stride=1, padding=1),
#             nn.BatchNorm2d(hidden_units*2),
#             nn.ReLU(),
#             nn.Conv2d(in_channels=hidden_units*2, out_channels=hidden_units*2, kernel_size=3, stride=1, padding=1),
#             nn.BatchNorm2d(hidden_units*2),
#             nn.ReLU(),
#             nn.MaxPool2d(kernel_size=2)  # Reduces spatial dimensions by a factor of 2
#         )

#         self.decoder = nn.Sequential(
#             nn.ConvTranspose2d(in_channels=hidden_units*2, out_channels=hidden_units, kernel_size=2, stride=2),  # Upsample
#             nn.BatchNorm2d(hidden_units),
#             nn.ReLU(),
#             nn.ConvTranspose2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=2, stride=2),  # Upsample
#             nn.BatchNorm2d(hidden_units),
#             nn.ReLU(),
#             nn.Conv2d(in_channels=hidden_units, out_channels=output_channels, kernel_size=1)  # Output channels
#         )


#     def _initialize_weights(self):
#         for m in self.modules():
#             if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):
#                 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
#                 if m.bias is not None:
#                     nn.init.constant_(m.bias, 0)
#             elif isinstance(m, nn.BatchNorm2d):
#                 nn.init.constant_(m.weight, 1)
#                 nn.init.constant_(m.bias, 0)
#         # Initialize weights
#         self._initialize_weights()

#     def forward(self, x):
#         x = self.conv_block_1(x)
#         x = self.conv_block_2(x)
#         x = self.decoder(x)
#         return x

# # Instantiate the model
# model_0 = CNN_Model_V0(input_channels=3, hidden_units=16, output_channels=1)

# # Create a dummy input tensor
# dummy_input = torch.randn(1, 3, 128, 128) # Batch size 1, 3 channels, 128x128 image

# # Pass the dummy input through the model
# output = model_0(dummy_input)


# # Apply sigmoid activation to the output
# output = torch.sigmoid(output)

# #Apply a binary mask
# binary_mask = (output > 0.5).float()
# print(binary_mask)

# # Convert the bianry_mask to numpy for visualization
# binary_image = binary_mask.squeeze(0).squeeze(0).detach().numpy()  # Remove batch size and channel dimensions

# print(f"Shape of the output: {output.shape}")
# print(f"Shape of the binary image: {binary_image.shape}")



# # Display the output
# fig, ax = plt.subplots(1, 2, figsize=(12, 6))
# ax[0].imshow(binary_image, cmap='gray')
# ax[0].set_title("Output")
# ax[0].axis('off')

# ax[1].imshow(dummy_input.squeeze(0).permute(1,2,0).detach().numpy(), cmap = "gray")
# ax[1].set_title("Dummy Input")
# ax[1].axis('off')

# plt.show()

# loss_fn = nn.BCEWithLogitsLoss()
# optimizer = torch.optim.SGD(model_0.parameters(), lr = 0.01)

# device = "cuda" if torch.cuda.is_available() else "cpu"

# import torch
# from torch import nn
# import torch.nn.functional as F
# from tqdm.auto import tqdm
# from timeit import default_timer as timer
# import matplotlib.pyplot as plt

# # Define the pixel accuracy, dice coefficient, and IoU functions
# def pixel_accuracy(y_true, y_pred):
#     y_pred = torch.sigmoid(y_pred)  # Apply sigmoid to get probabilities
#     y_pred = (y_pred > 0.5).float()  # Convert probabilities to binary
#     correct = (y_pred == y_true).float().sum()
#     total = y_true.numel()
#     accuracy = correct / total
#     return accuracy.item()

# def dice_coefficient(y_true, y_pred):
#     y_pred = torch.sigmoid(y_pred)  # Apply sigmoid to get probabilities
#     y_pred = (y_pred > 0.5).float()  # Convert probabilities to binary
#     y_true = y_true.view(-1)
#     y_pred = y_pred.view(-1)
#     intersection = (y_true * y_pred).sum()
#     union = y_true.sum() + y_pred.sum()
#     dice = 2. * intersection / (union + 1e-8)  # Add epsilon to avoid division by zero
#     return dice.item()

# def iou(y_true, y_pred):
#     y_pred = torch.sigmoid(y_pred)  # Apply sigmoid to get probabilities
#     y_pred = (y_pred > 0.5).float()  # Convert probabilities to binary
#     y_true = y_true.view(-1)
#     y_pred = y_pred.view(-1)
#     intersection = (y_true * y_pred).sum()
#     union = (y_true + y_pred).sum() - intersection
#     iou = intersection / (union + 1e-8)  # Add epsilon to avoid division by zero
#     return iou.item()

# # Define the print_train_time function
# def print_train_time(start: float, end: float, device: torch.device):
#     total_time = end - start
#     print(f"Training time on {device}: {total_time:.3f} seconds")


# # Optionally, visualize some predictions and ground truths for debugging
# def visualize_predictions(model, data_loader, device):
#     model.eval()
#     with torch.no_grad():
#         for images, masks in data_loader:
#             images, masks = images.to(device), masks.to(device)
#             outputs = model(images)
#             outputs = torch.sigmoid(outputs)  # Apply sigmoid to get probabilities
#             outputs = (outputs > 0.5).float()  # Convert probabilities to binary

#             for i in range(min(len(images), 5)):  # Visualize a few images
#                 plt.figure(figsize=(12, 4))

#                 plt.subplot(1, 3, 1)
#                 plt.title('Input Image')
#                 plt.imshow(images[i].cpu().permute(1, 2, 0))  # Convert to HWC format
#                 plt.axis('off')

#                 plt.subplot(1, 3, 2)
#                 plt.title('Ground Truth')
#                 plt.imshow(masks[i].cpu().squeeze(), cmap='gray')  # Squeeze to remove channel dimension
#                 plt.axis('off')

#                 plt.subplot(1, 3, 3)
#                 plt.title('Model Output')
#                 plt.imshow(outputs[i].cpu().squeeze(), cmap='gray')  # Squeeze to remove channel dimension
#                 plt.axis('off')

#                 plt.show()
#             break  # Show only one batch for visualization

"""###Training step"""

# import torch
# from torch import nn
# from tqdm.auto import tqdm
# from timeit import default_timer as timer

# # Define the training step
# def train_step(model: nn.Module,
#                data_loader: torch.utils.data.DataLoader,
#                loss_fn: nn.Module,
#                optimizer: torch.optim.Optimizer,
#                accuracy_fn,
#                device: torch.device):
#     model.train()  # Set model to training mode
#     running_loss = 0.0
#     running_accuracy = 0.0

#     for images, masks in data_loader:
#         images, masks = images.to(device), masks.to(device)

#         # Forward pass
#         outputs = model(images)
#         outputs_sigmoid = outputs.sigmoid()
#         outputs_thresholded = (outputs_sigmoid > 0.5).float()
#         loss = loss_fn(outputs, masks)

#         # Backward pass and optimization
#         optimizer.zero_grad()
#         loss.backward()
#         optimizer.step()

#         # Compute running loss
#         running_loss += loss.item() * images.size(0)  # Multiply by batch size for average loss

#         # Compute accuracy
#         accuracy = accuracy_fn(masks, outputs_thresholded)
#         running_accuracy += accuracy * images.size(0)  # Multiply by batch size for average accuracy

#     # Calculate average loss and accuracy
#     train_loss = running_loss / len(data_loader.dataset)
#     train_accuracy = running_accuracy / len(data_loader.dataset)

#     print(f"Train loss: {train_loss:.4f} | Train accuracy: {train_accuracy * 100:.2f}%")

"""###Testing step"""

# # Define the testing step
# def test_step(model: nn.Module,
#               data_loader: torch.utils.data.DataLoader,
#               loss_fn: nn.Module,
#               accuracy_fn,
#               device: torch.device) -> dict:
#     model.eval()  # Set model to evaluation mode
#     running_loss = 0.0
#     running_accuracy = 0.0

#     with torch.no_grad():  # No need to calculate gradients
#         for images, masks in data_loader:
#             images, masks = images.to(device), masks.to(device)


#         # Forward pass
#         outputs = model(images)
#         outputs_sigmoid = outputs.sigmoid()
#         outputs_thresholded = (outputs_sigmoid > 0.5).float()
#         loss = loss_fn(outputs, masks)

#         # Compute running loss
#         running_loss += loss.item() * images.size(0)  # Multiply by batch size for average loss

#         # Compute accuracy
#         accuracy = accuracy_fn(masks, outputs_thresholded)
#         running_accuracy += accuracy * images.size(0)  # Multiply by batch size for average accuracy

#     # Calculate average loss and accuracy
#     test_loss = running_loss / len(data_loader.dataset)
#     test_accuracy = running_accuracy / len(data_loader.dataset)

#     print(f"Test loss: {test_loss:.4f} | Test accuracy: {test_accuracy * 100:.2f}%")

"""###Training looop"""

# # Training loop
# start_time = timer()
# epochs = 3

# for epoch in tqdm(range(epochs)):
#     print(f"Epoch {epoch + 1}/{epochs}:")
#     train_step(model=model_0,
#                data_loader=train_loader,
#                loss_fn=loss_fn,
#                optimizer=optimizer,
#                accuracy_fn=pixel_accuracy,
#                device=device)
#     test_step(model=model_0,
#               data_loader=test_loader,
#               loss_fn=loss_fn,
#               accuracy_fn=pixel_accuracy,
#               device=device)

# end_time = timer()
# print(f"Training time: {end_time - start_time:.2f} seconds")

# visualize_predictions(model_0, test_loader, device)

"""###////////////////////////////////////
###////////////////////////////////////
###////////////////////////////////////

##Creating a Classification model that predicts Polyps

Classification is a great stepping stone to object detection.
"""

#Paths

#/content/data/PolypGen2021_MultiCenterData_v3/sequenceData/Negative/*/

#/content/data/PolypGen2021_MultiCenterData_v3/sequenceData/Positive/*/

"""###Hyperparameters for model"""

#Hyperparameters
lr= 0.0005
num_workers = 2
input_channels = 3
hidden_units = 32
num_batches = 16
epochs = 3
image_size = 128
device = "cuda" if torch.cuda.is_available() else "cpu"

"""###Organizing the dataset"""

import os
import shutil
from glob import glob
import random

# Directories
source_dir_positive = '/content/data/PolypGen2021_MultiCenterData_v3/sequenceData/positive/*/images_seq*'
source_dir_negative = '/content/data/PolypGen2021_MultiCenterData_v3/sequenceData/negativeOnly/*'
target_dir = '/content/data/classification'

# Create target directories
os.makedirs(f'{target_dir}/train/positive', exist_ok=True)
os.makedirs(f'{target_dir}/train/negative', exist_ok=True)
os.makedirs(f'{target_dir}/test/positive', exist_ok=True)
os.makedirs(f'{target_dir}/test/negative', exist_ok=True)

# Helper function to split data
def split_data(source_paths, train_split=0.8):
    train_paths = []
    test_paths = []
    for path in source_paths:
        images = glob(os.path.join(path, '*.jpg'))
        random.shuffle(images)
        split_idx = int(len(images) * train_split)
        train_paths.extend(images[:split_idx])
        test_paths.extend(images[split_idx:])
    return train_paths, test_paths

# Split positive images
positive_paths = glob(source_dir_positive)
train_positive, test_positive = split_data(positive_paths)

# Split negative images
negative_paths = glob(source_dir_negative)
train_negative, test_negative = split_data(negative_paths)

# Function to copy images
def copy_images(image_paths, target_folder):
    for img_path in image_paths:
        shutil.copy(img_path, target_folder)

# Copy images to target directories
copy_images(train_positive, f'{target_dir}/train/positive')
copy_images(test_positive, f'{target_dir}/test/positive')
copy_images(train_negative, f'{target_dir}/train/negative')
copy_images(test_negative, f'{target_dir}/test/negative')

print("Dataset organized successfully.")

"""###Creating dataset and dataloaders"""

import os
from glob import glob
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image

class PolypDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.image_paths = []
        self.labels = []

        # Collect all image paths and labels
        for label, class_name in enumerate(['negative', 'positive']):
            class_dir = os.path.join(root_dir, class_name)
            if not os.path.exists(class_dir):
                print(f"Directory not found: {class_dir}")
                continue
            img_paths = glob(os.path.join(class_dir, '*.jpg'))
            if len(img_paths) == 0:
                print(f"Found 0 images in {class_dir}")
            self.image_paths.extend(img_paths)
            self.labels.extend([label] * len(img_paths))

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        image = Image.open(img_path).convert('RGB')
        label = self.labels[idx]

        if self.transform:
            image = self.transform(image)

        return image, label

# Define transformations
#Training transformation
train_transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ToTensor(),
])
#Testing transformation
test_transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
])



# Define the correct directories
train_dir = '/content/data/classification/train'
test_dir = '/content/data/classification/test'

# Create datasets and dataloaders
train_dataset = PolypDataset(root_dir=train_dir, transform=train_transform)
test_dataset = PolypDataset(root_dir=test_dir, transform=test_transform)

print(f"Number of training samples: {len(train_dataset)}")
print(f"Number of testing samples: {len(test_dataset)}")

train_loader = DataLoader(train_dataset, batch_size=num_batches, shuffle=True, num_workers=num_workers)
test_loader = DataLoader(test_dataset, batch_size=num_batches, shuffle=False, num_workers=num_workers)

"""###Model architecture"""

import torch
from torch import nn

class BinaryClassificationCNN(nn.Module):
    def __init__(self, input_channels: int, hidden_units: int):
        super(BinaryClassificationCNN, self).__init__()
        self.conv_block_1 = nn.Sequential(
            nn.Conv2d(in_channels=input_channels, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)
        )
        self.conv_block_2 = nn.Sequential(
            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units * 2, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=hidden_units * 2, out_channels=hidden_units * 2, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)
        )

        # To calculate the size after convolutional layers
        self._calculate_linear_input_size()

        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(in_features=self.flattened_size, out_features=1)  # Output dimension is 1 for binary classification
        )

    def _calculate_linear_input_size(self):
        # Create a dummy input tensor
        dummy_input = torch.zeros(1, 3, 128, 128)  # Example input shape: batch size 1, 3 channels, 128x128 image
        dummy_output = self.conv_block_1(dummy_input)
        dummy_output = self.conv_block_2(dummy_output)
        self.flattened_size = dummy_output.numel()  # Number of features after flattening

    def forward(self, x):
        x = self.conv_block_1(x)
        x = self.conv_block_2(x)
        x = self.classifier(x)
        return x

model = BinaryClassificationCNN(3, 16)

"""###Loading a batch of data and visualizing the outputs"""

import matplotlib.pyplot as plt
import torch

# Ensure the model is on the same device as the data
model = model.to(device)

# Get a batch of images from the test_loader
data_iter = iter(test_loader)
images, labels = next(data_iter)

# Move images and labels to the device
images = images.to(device)
labels = labels.to(device)

# Pass the images through the model to get the outputs
model.eval()
with torch.no_grad():
    outputs = model(images)
    outputs_sigmoid = torch.sigmoid(outputs)

# Plot the sample images and their corresponding outputs
fig, axes = plt.subplots(2, 4, figsize=(16, 8))
axes = axes.ravel()

for idx in range(min(8, len(images))):  # Ensure we do not exceed the number of images
    img = images[idx].cpu().permute(1, 2, 0).numpy()  # Convert to numpy array and move channels to last dimension
    img = (img - img.min()) / (img.max() - img.min())  # Normalize image to [0, 1] for display
    output_value = outputs_sigmoid[idx].item()
    pred_label = "Positive" if output_value > 0.5 else "Negative"
    axes[idx].imshow(img)  # Remove cmap='gray' for RGB images
    axes[idx].set_title(f"Model Output: {output_value:.4f}\nPrediction: {pred_label}")
    axes[idx].axis('off')

# Hide unused subplots if fewer than 8 images
for idx in range(len(images), 8):
    axes[idx].axis('off')

plt.show()

"""###Timer, Loss function, optimizer, and accuracy function"""

import torch
from torch import nn, optim
from timeit import default_timer as timer

# Timer function
def timer_start():
    return timer()

def timer_end(start_time):
    end_time = timer()
    return end_time - start_time

# Define the loss function
loss_fn = nn.BCEWithLogitsLoss()  # Suitable for binary classification with logits

# Define the optimizer
optimizer = torch.optim.SGD(model.parameters(), lr = lr)

# Define the accuracy function
def accuracy_fn(y_true, y_pred):
    # Convert predictions to binary
    y_pred = (y_pred > 0.5).float()
    correct = (y_true == y_pred).float()
    accuracy = correct.sum() / correct.numel()
    return accuracy

"""###Model Architecture: CNN.
Establishing hte model's architecture.

Training the model with training and testing specific function

Using a training loop function on the model
"""

import torch
from torch import nn, optim
from torch.utils.data import DataLoader
from tqdm.auto import tqdm
from timeit import default_timer as timer
import matplotlib.pyplot as plt

# Define the model
class BinaryClassificationCNN(nn.Module):
    def __init__(self, input_channels: int, hidden_units: int):
        super(BinaryClassificationCNN, self).__init__()
        self.conv_block_1 = nn.Sequential(
            nn.Conv2d(in_channels=input_channels, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)
        )
        self.conv_block_2 = nn.Sequential(
            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units * 2, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=hidden_units * 2, out_channels=hidden_units * 2, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)
        )
        self._calculate_linear_input_size()
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(in_features=self.flattened_size, out_features=1)  # Output dimension is 1 for binary classification
        )

    def _calculate_linear_input_size(self):
        dummy_input = torch.zeros(1, 3, 128, 128)  # Example input shape: batch size 1, 3 channels, 128x128 image
        dummy_output = self.conv_block_1(dummy_input)
        dummy_output = self.conv_block_2(dummy_output)
        self.flattened_size = dummy_output.numel()  # Number of features after flattening

    def forward(self, x):
        x = self.conv_block_1(x)
        x = self.conv_block_2(x)
        x = self.classifier(x)
        return x

# Define train and test steps
def train_step(model: nn.Module,
               data_loader: DataLoader,
               loss_fn: nn.Module,
               optimizer: optim.Optimizer,
               accuracy_fn,
               device: torch.device):
    model.train()  # Set model to training mode
    running_loss = 0.0
    running_accuracy = 0.0

    for images, labels in data_loader:
        images, labels = images.to(device), labels.to(device).float()  # Convert labels to float

        # Forward pass
        outputs = model(images)
        loss = loss_fn(outputs, labels.unsqueeze(1))  # Add dimension to labels

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Compute running loss
        running_loss += loss.item() * images.size(0)  # Multiply by batch size for average loss

        # Compute accuracy
        accuracy = accuracy_fn(labels, torch.sigmoid(outputs))
        running_accuracy += accuracy * images.size(0)  # Multiply by batch size for average accuracy

    # Calculate average loss and accuracy
    train_loss = running_loss / len(data_loader.dataset)
    train_accuracy = running_accuracy / len(data_loader.dataset)

    print(f"Train loss: {train_loss:.4f} | Train accuracy: {train_accuracy * 100:.2f}%")

def test_step(model: nn.Module,
              data_loader: DataLoader,
              loss_fn: nn.Module,
              accuracy_fn,
              device: torch.device) -> dict:
    model.eval()  # Set model to evaluation mode
    running_loss = 0.0
    running_accuracy = 0.0

    with torch.no_grad():  # No need to calculate gradients
        for images, labels in data_loader:
            images, labels = images.to(device), labels.to(device).float()  # Convert labels to float

            # Forward pass
            outputs = model(images)
            loss = loss_fn(outputs, labels.unsqueeze(1))  # Add dimension to labels

            # Compute running loss
            running_loss += loss.item() * images.size(0)  # Multiply by batch size for average loss

            # Compute accuracy
            accuracy = accuracy_fn(labels, torch.sigmoid(outputs))
            running_accuracy += accuracy * images.size(0)  # Multiply by batch size for average accuracy

    # Calculate average loss and accuracy
    test_loss = running_loss / len(data_loader.dataset)
    test_accuracy = running_accuracy / len(data_loader.dataset)

    print(f"Test loss: {test_loss:.4f} | Test accuracy: {test_accuracy * 100:.2f}%")

    return {"test_loss": test_loss, "test_accuracy": test_accuracy}

# Define the training and evaluation loop
def train_and_evaluate(model, train_loader, test_loader, loss_fn, optimizer, accuracy_fn, device, epochs):
    start_time = timer()

    for epoch in tqdm(range(epochs)):
        print(f"Epoch {epoch + 1}/{epochs}:")

        # Training step
        train_step(model=model,
                   data_loader=train_loader,
                   loss_fn=loss_fn,
                   optimizer=optimizer,
                   accuracy_fn=accuracy_fn,
                   device=device)

        # Testing step
        test_step(model=model,
                  data_loader=test_loader,
                  loss_fn=loss_fn,
                  accuracy_fn=accuracy_fn,
                  device=device)

        # Check for any updates in the model
        print("Model parameters (sample):")
        for name, param in model.named_parameters():
            if param.requires_grad:
                print(f"{name}: {param.grad.mean().item() if param.grad is not None else 'No grad'}")

    end_time = timer()
    print(f"Training time: {end_time - start_time:.2f} seconds")

# Example usage
# Initialize the model, optimizer, loss function, and accuracy function
device = "cuda" if torch.cuda.is_available() else "cpu"
model = BinaryClassificationCNN(input_channels=input_channels, hidden_units=hidden_units).to(device)
optimizer = optim.Adam(model.parameters(), lr=lr)
loss_fn = nn.BCEWithLogitsLoss()  # Appropriate loss function for logits
accuracy_fn = lambda labels, preds: ((preds > 0.5).float() == labels.unsqueeze(1)).float().mean().item()

# Define data loaders
# Replace `train_loader` and `test_loader` with your actual DataLoader objects
# For demonstration, Iâ€™m assuming they are already defined correctly

# Train and evaluate
epochs = epochs  # Use fewer epochs for testing
train_and_evaluate(model, train_loader, test_loader, loss_fn, optimizer, accuracy_fn, device, epochs)

import matplotlib.pyplot as plt
import torch
import numpy as np

def visualize_predictions(model, data_loader, device, num_images=9):
    model.eval()  # Set model to evaluation mode

    # Collect all images and labels from the data_loader
    all_images = []
    all_labels = []

    for images, labels in data_loader:
        all_images.extend(images)
        all_labels.extend(labels)

    all_images = torch.stack(all_images)
    all_labels = torch.tensor(all_labels).float()

    # Randomly select indices for visualization
    indices = np.random.choice(len(all_images), num_images, replace=False)

    # Move selected images and labels to the device
    selected_images = all_images[indices].to(device)
    selected_labels = all_labels[indices].to(device)

    with torch.no_grad():  # No need to calculate gradients
        outputs = model(selected_images)
        outputs_sigmoid = torch.sigmoid(outputs)

    # Set up the plot
    fig, axes = plt.subplots(3, 3, figsize=(18, 18))
    axes = axes.ravel()

    for idx in range(num_images):
        img = selected_images[idx].cpu().permute(1, 2, 0).numpy()  # Convert to numpy array
        img = (img - img.min()) / (img.max() - img.min())  # Normalize image to [0, 1]

        true_label = "Positive" if selected_labels[idx].item() > 0.5 else "Negative"
        pred_prob = outputs_sigmoid[idx].item()
        pred_label = "Positive" if pred_prob > 0.5 else "Negative"

        axes[idx].imshow(img)
        axes[idx].set_title(f"True: {true_label}\tPred: {pred_label}\tProb: {pred_prob:.4f}")
        axes[idx].axis('off')  # Hide axis

    # Hide unused subplots if fewer than num_images
    for idx in range(num_images, 9):
        axes[idx].axis('off')

    plt.show()

# Example usage
visualize_predictions(model, test_loader, device, num_images=9)

import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

def calculate_confusion_matrix(model, data_loader, device):
    model.eval()  # Set model to evaluation mode

    all_preds = []
    all_labels = []

    with torch.no_grad():
        for images, labels in data_loader:
            images, labels = images.to(device), labels.to(device).float()  # Convert labels to float
            outputs = model(images)
            preds = torch.sigmoid(outputs).cpu().numpy()  # Apply sigmoid and move to CPU
            all_preds.extend(preds)
            all_labels.extend(labels.cpu().numpy())

    all_preds = np.array(all_preds).flatten()
    all_labels = np.array(all_labels).flatten()

    # Convert probabilities to binary predictions
    all_preds_binary = (all_preds > 0.5).astype(int)

    # Compute confusion matrix
    cm = confusion_matrix(all_labels, all_preds_binary, labels=[0, 1])

    # Reorder confusion matrix: TP, FP, FN, TN
    tp, fp, fn, tn = cm.ravel()

    return tn, fp, fn, tp

def plot_confusion_matrix(cm, labels):
    # Plot confusion matrix with desired layout
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    fig, ax = plt.subplots(figsize=(8, 8))
    disp.plot(ax=ax, cmap=plt.cm.Blues, values_format='d', include_values=True)
    plt.title('Confusion Matrix')
    plt.show()

# Example usage
tn, fp, fn, tp = calculate_confusion_matrix(model, test_loader, device)
print(f"True Negatives: {tn}")
print(f"False Positives: {fp}")
print(f"False Negatives: {fn}")
print(f"True Positives: {tp}")

# Create confusion matrix with TP in top left, TN in bottom right
labels = ['Positive', 'Negative']  # Define labels for visualization
cm = np.array([[tp, fp], [fn, tn]])
plot_confusion_matrix(cm, labels)

"""###Evaluating the model with differrent data in case of overfitting"""

# Define transformations
from torchvision import transforms
transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.RandomHorizontalFlip(p=1),
    transforms.ToTensor(),
])

# Define the correct directories
train_dir = '/content/data/classification/train'
eval_dir = '/content/data/classification/test'

# Create datasets and dataloaders for evaluation
#train_dataset = PolypDataset(root_dir=train_dir, transform=transform)
eval_dataset = PolypDataset(root_dir=eval_dir, transform=transform)

#print(f"Number of training samples: {len(train_dataset)}")
print(f"Number of testing samples: {len(eval_dataset)}")

#train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)
eval_loader = DataLoader(eval_dataset, batch_size=16, shuffle=True, num_workers=2)

tn, fp, fn, tp = calculate_confusion_matrix(model, eval_loader, device)
print(f"True Negatives: {tn}")
print(f"False Positives: {fp}")
print(f"False Negatives: {fn}")
print(f"True Positives: {tp}")

# Create confusion matrix with TP in top left, TN in bottom right
labels = ['Positive', 'Negative']  # Define labels for visualization
cm = np.array([[tp, fp], [fn, tn]])
plot_confusion_matrix(cm, labels)

(tp + tn) / (tp + tn + fp +fn) #Print the accuracy

model.state_dict()

"""###Saving model path"""

from pathlib import Path

#Create directory path

MODEL_PATH = Path("models")
MODEL_PATH.mkdir(parents=True,
                 exist_ok=True)

#Create model save
MODEL_NAME = "Polyp_AI_Model_V0.pth"
MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME

#Save the model state dict
print(f"Saving model to: {MODEL_SAVE_PATH}")
torch.save(obj=model.state_dict(),
           f=MODEL_SAVE_PATH)





